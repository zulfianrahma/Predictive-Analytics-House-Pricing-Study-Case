# -*- coding: utf-8 -*-
"""Proyek Pertama : Predictive Analytics - Zulfian Rahmadiansyah.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ia11OdKMEKmBCyV3v_GVyFZILJt6OTHO

#1. Installing Dependencies

*   Instalasi **opendatasets** untuk proses import data dari Kaggle
"""

!pip install opendatasets

"""*   Instalasi *library* yang diperlukan"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns
import opendatasets as od

"""# 2. Import Dataset

*   Import *dataset* dari Kaggle (diperlukan *username* dan *key* dari akun Kaggle selama proses import *dataset*)
"""

# import dataset
od.download('https://www.kaggle.com/datasets/harlfoxem/housesalesprediction')

"""*   Membaca berkas csv dari *dataset*"""

df = pd.read_csv('/content/housesalesprediction/kc_house_data.csv')
df

"""*   Melihat informasi terkait *dataset*"""

df.info()

"""*   Melihat deksripsi statistik dari *dataset*"""

df.describe()

"""## Column defintions

*   id - Unique ID for each home sold
*   date - Date of the home sale
*   price - Price of each home sold
*   bedrooms - Number of bedrooms
*   bathrooms - Number of bathrooms, where .5 accounts for a room with a toilet but no shower
*   sqft_living - Square footage of the apartments interior living space
*   sqft_lot - Square footage of the land space
*   floors - Number of floors
*   waterfront - A dummy variable for whether the apartment was overlooking the waterfront or not
*   view - An index from 0 to 4 of how good the view of the property was: 0 = No view, 1 = Fair 2 = Average, 3 = Good, 4 = Excellent
*   condition - An index from 1 to 5 on the condition of the apartment: 1 = Poor- Worn out, 2 = Fair- Badly worn, 3 = Average, 4 = Good, 5= Very Good
*   grade - An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design.
*   sqft_above - The square footage of the interior housing space that is above ground level
*   sqft_basement - The square footage of the interior housing space that is below ground level
*   yr_built - The year the house was initially built
*   yr_renovated - The year of the houseâ€™s last renovation
*   zipcode - What zipcode area the house is in
*   lat - Lattitude
*   long - Longitude
*   sqft_living15 - The square footage of interior housing living space for the nearest 15 neighbors
*   sqft_lot15 - The square footage of the land lots of the nearest 15 neighbors

# 3. Data Preparation

*   Mengubah data numerik menjadi data kategori pada kolom "view"
  *   Proses tersebut dilakukan supaya data tersebut dapat dimasukkan dalam teknik *one-hot-encoding*
"""

# Use replace() to change specific numeric values to labels
mapping_view = {
    0: 'No view',
    1: 'Fair',
    2: 'Average',
    3: 'Good',
    4: 'Excelent'
}

df['view'] = df['view'].replace(mapping_view)

"""

*   Mengubah data numerik menjadi data kategori pada kolom "condition"
  *   Proses tersebut dilakukan supaya data tersebut dapat dimasukkan dalam teknik *one-hot-encoding*
"""

mapping_condition = {
    1: 'Poor- Worn out',
    2: 'Fair- Badly worn',
    3: 'Average',
    4: 'Good',
    5: 'Very Good'
}

df['condition'] = df['condition'].replace(mapping_condition)

"""

*   Mengubah data numerik menjadi data kategori pada kolom "grade" dan menyimpan hasil perubahan tersebut pada kolom baru yang bernama "grade_category"
  *   Proses tersebut dilakukan supaya data tersebut dapat dimasukkan dalam teknik *one-hot-encoding*
"""

# Define the bin edges and labels for the categorical feature
bin_edges_grade = [0, 3, 6, 10, 13]
bin_labels_grade = ['Bad', 'Okay', 'Better', 'Excelent']

# Use pd.cut() to create a new categorical column based on the 'grade' column
df['grade_category'] = pd.cut(df['grade'], bins=bin_edges_grade, labels=bin_labels_grade)

"""

*   Menghilangkan kolom "grade" karena nilai datanya telah digantikan oleh kolom "grade_category"
"""

df.drop(['grade'], inplace=True, axis=1)

"""

*   Menghilangkan kolom "id", "date", "zipcode", "lat", dan "long" karena data dari kolom tersebut dirasa tidak diperlukan dalam proses pengembangkan model yang dirancang
"""

# untuk penyederhanaan, ada beberapa kolom yang dihilangkan
dropped_colomn = ['id', 'date', 'zipcode', 'lat', 'long']

df.drop(columns=dropped_colomn, inplace=True)

"""

*   Mengubah data numerik menjadi data kategori pada kolom "yr_renovated" dan "waterfront".
*   Hasil perubahan pada kolom "yr_renovated" akan dimasukkan pada kolom baru yang bernama "renovated".
*   Hasil perubahan pada kolom "waterfront" akan memperbaharui data pada kolom tersebut.
  *   Proses tersebut dilakukan supaya data tersebut dapat dimasukkan dalam teknik *one-hot-encoding*"""

# untuk penyederhanaan, ada beberapa nilai pada kolom tertentu yang diubah dari data numerik menjadi data kelompok
df['renovated'] = df['yr_renovated'].apply(lambda x: 'Yes' if x != 0 else 'No')
df['waterfront'] = df['waterfront'].apply(lambda x: 'Yes' if x != 0 else 'No')

"""

*   Menghilangkan kolom "yr_renovated" karena nilai datanya telah digantikan oleh kolom "renovated"
"""

df.drop(['yr_renovated'], inplace=True, axis=1)

"""

*   Melihat informasi terkait dataset untuk pengecekan
"""

df.info()

"""*   Visualisasi data numerik untuk mendeteksi outlier menggunakan boxplot pada fitur "bedrooms"
"""

sns.boxplot(x=df['bedrooms'])

"""*   Visualisasi data numerik untuk mendeteksi outlier menggunakan boxplot pada fitur "bathrooms"
"""

sns.boxplot(x=df['bathrooms'])

"""*   Visualisasi data numerik untuk mendeteksi outlier menggunakan boxplot pada fitur "sqft_living"
"""

sns.boxplot(x=df['sqft_living'])

"""*   Visualisasi data numerik untuk mendeteksi outlier menggunakan boxplot pada fitur "sqft_lot"
"""

sns.boxplot(x=df['sqft_lot'])

"""*   Visualisasi data numerik untuk mendeteksi outlier menggunakan boxplot pada fitur "floors"
"""

sns.boxplot(x=df['floors'])

"""*   Visualisasi data numerik untuk mendeteksi outlier menggunakan boxplot pada fitur "sqft_above"
"""

sns.boxplot(x=df['sqft_above'])

"""*   Visualisasi data numerik untuk mendeteksi outlier menggunakan boxplot pada fitur "sqft_basement"
"""

sns.boxplot(x=df['sqft_basement'])

"""*   Visualisasi data numerik untuk mendeteksi outlier menggunakan boxplot pada fitur "yr_built"
"""

sns.boxplot(x=df['yr_built'])

"""*   Visualisasi data numerik untuk mendeteksi outlier menggunakan boxplot pada fitur "sqft_living15"
"""

sns.boxplot(x=df['sqft_living15'])

"""*   Visualisasi data numerik untuk mendeteksi outlier menggunakan boxplot pada fitur "sqft_lot15"
"""

sns.boxplot(x=df['sqft_lot15'])

"""

*   Melihat ukuran dataset sebelum dilakukan teknik IQR (*Inter Quartile Range*) untuk menghilangkan *outlier*"""

df.shape

"""

*   Menghilangkan *outlier* dari data - data numerik menggunakan teknik IQR (*Inter Quartile Range*)"""

Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR=Q3-Q1

"""

*   Memisahkan data numerik dengan data data kategori untuk digunakan pada teknik IQR (*Inter Quartile Range*)
"""

# Convert the Categorical columns to NumPy arrays for comparison
df_num = df.select_dtypes(include=[np.number])
df_cat = df.select_dtypes(exclude=[np.number])

"""

*   Menghilangkan *outlier* menggunakan IQR (*Inter Quartile Range*)
"""

df = df[~((df_num<(Q1-1.5*IQR))|(df_num>(Q3+1.5*IQR))).any(axis=1)]

"""

*   Melihat ukuran dataset setelah dilakukan teknik IQR (*Inter Quartile Range*) untuk menghilangkan *outlier*"""

# Cek ukuran dataset setelah kita drop outliers
df.shape

"""

*   Melihat informasi terkait dataset untuk pengecekan"""

df.info()

"""

*   Mengelompokkan data numerik pada variabel "numerical features"
*   Mengelompokkan data kategori pada variabel "categorical_features"
"""

numerical_features = ['price', 'bedrooms', 'bathrooms','sqft_living', 'sqft_lot', 'floors', 'sqft_above', 'sqft_basement', 'yr_built', 'sqft_living15', 'sqft_lot15']
categorical_features = ['waterfront', 'view', 'condition', 'renovated', 'grade_category']

"""

*   Melakukan pengecekan dengan cara menghitung jumlah data pada variabel "numerical_features" dan "categorical_features"
"""

print('Jumlah data numerik pada variabel "numerical_features" adalah',len(numerical_features))
print('Jumlah data kategori pada variabel "categorical_features" adalah',len(categorical_features))

"""*   Melakukan analisa pada fitur "waterfront" meliputi jumlah sampel dan persentase tiap kategori

"""

feature = categorical_features[0]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
data0 = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(data0)
count.plot(kind='bar', title=feature);

"""*   Melakukan analisa pada fitur "view" meliputi jumlah sampel dan persentase tiap kategori"""

feature = categorical_features[1]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
data1 = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(data1)
count.plot(kind='bar', title=feature);

"""*   Melakukan analisa pada fitur "condition" meliputi jumlah sampel dan persentase tiap kategori"""

feature = categorical_features[2]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
data2 = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(data2)
count.plot(kind='bar', title=feature);

"""*   Melakukan analisa pada fitur "renovated" meliputi jumlah sampel dan persentase tiap kategori"""

feature = categorical_features[3]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
data3 = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(data3)
count.plot(kind='bar', title=feature);

"""*   Melakukan analisa pada fitur "grade_category" meliputi jumlah sampel dan persentase tiap kategori"""

feature = categorical_features[4]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
data4 = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(data4)
count.plot(kind='bar', title=feature);

"""

*   Membuat histogram pada tiap fitur numerik pada variabel "numerical_features"
"""

df.hist(bins=50, figsize=(20,15))
plt.show()

"""

*   Mengecek rata-rata harga rumah terhadap masing-masing fitur untuk mengetahui pengaruh fitur kategori terhadap harga rumah"""

cat_features = df.select_dtypes(include=['object', 'category']).columns.to_list()

for col in cat_features:
  sns.catplot(x=col, y="price", kind="bar", dodge=False, height = 4, aspect = 3,  data=df, palette="Set3")
  plt.title("Rata-rata 'price' relatif terhadap - '{}'".format(col))

"""

*   Mengamati hubungan antar fitur numerik dengan fungsi **pairplot()**"""

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
sns.pairplot(df, diag_kind = 'kde')

"""

*   Mengobservasi korelasi antara fitur numerik dengan fitur target menggunakan fungsi **corr()**"""

plt.figure(figsize=(10, 8))
correlation_matrix = df.corr().round(2)

# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""

*   Memilih 3 fitur numerik terbaik untuk dipertahankan berdasarkan hasil observasi korelasi antara fitur numerik dengan fitur target ("price"), yaitu:

|   **Fitur**   | **Koefisien korelasi** |
|:-------------:|:----------------------:|
|  sqft_living  |          0,56          |
|   sqft_above  |          0,45          |
| sqft_living15 |          0,49          |



*   Menghapus fitur numerik selain 3 fitur numerik yang dipertahankan karena tidak mempunyai korelasi yang signifikan terhadap fitur target ("price")

"""

dropped_colomn = ['bedrooms', 'bathrooms', 'sqft_lot', 'floors', 'sqft_basement', 'yr_built', 'sqft_lot15']

df.drop(columns=dropped_colomn, inplace=True)

"""

*   Melihat informasi terkait dataset untuk pengecekan"""

df.info()

"""

*   Melakukan proses encoding fitur kategori menggunakan teknik *one-hot-encoding*
"""

from sklearn.preprocessing import  OneHotEncoder

# Selecting both "object" and "category" data types as cat_features
cat_features = df.select_dtypes(include=['object', 'category']).columns.to_list()

# One-hot encode the categorical columns
df_encoded = pd.get_dummies(df, columns=cat_features)

"""

*   Mengubah dataframe awal menjadi dataframe yang sudah melalui proses encoding"""

df = df_encoded

"""

*   Melihat informasi terkait dataset untuk pengecekan

"""

df.info()

"""

*   Menampilkan 5 data teratas dataframe setelah proses *one-hot-encoding*"""

df.head()

"""

*   Mengamati hubungan antar fitur numerik yang tersisa dengan fungsi pairplot()"""

sns.pairplot(df[['sqft_living','sqft_above','sqft_living15']], plot_kws={"s": 3});

"""

*   Melakukan analisa PCA (*Principal Component Analysis*) pada fitur numerik tersisa sehingga dihasilkan proporsi dari tiap *principal component*"""

from sklearn.decomposition import PCA

pca = PCA(n_components=3, random_state=123)
pca.fit(df[['sqft_living','sqft_above', 'sqft_living15']])
princ_comp = pca.transform(df[['sqft_living','sqft_above', 'sqft_living15']])

"""

*   Melihat proporsi informasi dari ketiga komponen *principal component* yang dihasilkan"""

pca.explained_variance_ratio_.round(3)

"""

*   Melakukan analisa PCA (Principal Component Analysis) pada fitur numerik tersisa sehingga dihasilkan proporsi dari tiap *principal component*
*   Dipertahankan hasil *principal component* yang terbesar yaitu PC1 pada fitur baru yang bernama "sqrt_area"
"""

from sklearn.decomposition import PCA
pca = PCA(n_components=1, random_state=123)
pca.fit(df[['sqft_living','sqft_above', 'sqft_living15']])
df['sqft_area'] = pca.transform(df.loc[:, ('sqft_living','sqft_above', 'sqft_living15')]).flatten()

"""

*   Menghilangkan fitur "sqft_living", "sqft_above", dan "sqft_living15" karena sudah direduksi menggunakan teknik PCA"""

df.drop(['sqft_living','sqft_above', 'sqft_living15'], axis=1, inplace=True)

"""

*   Membagi dataset menjadi data latih (*train*) dan data uji (*test*) dengan proporsi data 90:10"""

from sklearn.model_selection import train_test_split

X = df.drop(["price"],axis =1)
y = df["price"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 123)

"""

*   Menghitung jumlah sampel pada seluruh data, data latih, dan data uji
"""

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""

*   Melakukan proses standarisasi fitur dengan mengurangkan mean (nilai rata-rata) kemudian membaginya dengan standar deviasi untuk menggeser distribusi yang dilakukan oleh fungsi **StandarScaler**
"""

from sklearn.preprocessing import StandardScaler

numerical_features = ['sqft_area']
scaler = StandardScaler()
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])
X_train[numerical_features].head()

"""

*   Melihat rincian informasi pada fitur numerik yang sudah melalui proses standarisasi dengan fungsi **StandarScaler**"""

X_train[numerical_features].describe().round(4)

"""# 4. Modelling

*   Mengimport library **GridSearchCV** untuk proses optimalisasi *hyperparameter* model
*   Mengimport library **mean_squared_error** (MSE) sebagai metrik evaluasi model yang digunakan
"""

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error

"""

*   Membuat tabel data yang akan menampung data berupa nama algoritma yang digunakan dan nilai MSE pada data latih dan data uji"""

models = pd.DataFrame(index=['train_mse', 'test_mse'],
                      columns=['KNN', 'RandomForest', 'Boosting'])

"""

*   Melakukan proses pelatihan model KNN pada data latih dengan bantuan GridSearchCV untuk memperoleh *hyperparameter* yang terbaik"""

from sklearn.neighbors import KNeighborsRegressor

# Create the KNN classifier
knn = KNeighborsRegressor()

# Define the hyperparameter grid to search
param_grid = {
    'n_neighbors': [3, 5, 7, 9],  # Number of neighbors to consider
    'weights': ['uniform', 'distance'],  # Weighting scheme for neighbors
    'p': [1, 2],  # Power parameter for the Minkowski metric (1 for Manhattan, 2 for Euclidean)
}

# Create Grid Search with Cross-Validation
grid_search = GridSearchCV(knn, param_grid, cv=5)

# Train the model on the training data with Grid Search
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

"""

*   Melakukan proses pelatihan model KNN pada data latih dengan hyperparameter terbaik dari pelatihan sebelumnya"""

# Create the KNN model with the best hyperparameters
knn_best = KNeighborsRegressor(**best_params)

# Train the model on the training data with the best hyperparameters
knn_best.fit(X_train, y_train)

models.loc['train_mse','KNN'] = mean_squared_error(y_pred = knn_best.predict(X_train), y_true=y_train)

"""

*   Melakukan proses pelatihan model Random Forest pada data latih dengan bantuan GridSearchCV untuk memperoleh *hyperparameter* yang terbaik"""

# Impor library yang dibutuhkan
from sklearn.ensemble import RandomForestRegressor

# Create the RandomForestRegressor
rf_regressor = RandomForestRegressor(random_state=42)

# Define the hyperparameter grid to search
param_grid = {
    'n_estimators': [50, 100],  # Number of trees in the forest
    'max_depth': [None, 5, 10],  # Maximum depth of the tree
    'min_samples_split': [2, 5],  # Minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2],  # Minimum number of samples required to be at a leaf node
}

# Create Grid Search with Cross-Validation
grid_search = GridSearchCV(rf_regressor, param_grid, cv=5)

# Train the model on the training data with Grid Search
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

"""

*   Melakukan proses pelatihan model Random Forest pada data latih dengan hyperparameter terbaik dari pelatihan sebelumnya"""

# Create the RandomForestRegressor with the best hyperparameters
rf_regressor_best = RandomForestRegressor(**best_params, random_state=42)

# Train the model on the training data with the best hyperparameters
rf_regressor_best.fit(X_train, y_train)

models.loc['train_mse','RandomForest'] = mean_squared_error(y_pred=rf_regressor_best.predict(X_train), y_true=y_train)

"""

*   Melakukan proses pelatihan model AdaBoost pada data latih dengan bantuan GridSearchCV untuk memperoleh *hyperparameter* yang terbaik"""

from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor

# Create the base estimator (decision tree)
base_estimator = DecisionTreeRegressor(max_depth=4)

# Create the AdaBoostRegressor
adaboost_regressor = AdaBoostRegressor(base_estimator=base_estimator, random_state=42)

# Define the hyperparameter grid to search
param_grid = {
    'n_estimators': [50, 100, 150],  # Number of estimators (weak learners)
    'learning_rate': [0.01, 0.1, 0.5, 1.0],  # Learning rate to adjust the contribution of each weak learner
}

# Create Grid Search with Cross-Validation
grid_search = GridSearchCV(adaboost_regressor, param_grid, cv=5)

# Train the model on the training data with Grid Search
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

"""

*   Melakukan proses pelatihan model AdaBoost pada data latih dengan hyperparameter terbaik dari pelatihan sebelumnya"""

# Create the AdaBoostRegressor with the best hyperparameters
adaboost_regressor_best = AdaBoostRegressor(base_estimator=base_estimator, **best_params, random_state=42)

# Train the model on the training data with the best hyperparameters
adaboost_regressor_best.fit(X_train, y_train)

models.loc['train_mse','Boosting'] = mean_squared_error(y_pred=adaboost_regressor_best.predict(X_train), y_true=y_train)

"""

*   Menampilkan tabel data yang memuat rangkuman nilai MSE pada pelatihan tiap algoritma menggunakan data latih
"""

models

"""# 5. Evaluate Model

*   Membuat tabel data yang akan menampung data berupa nama model yang digunakan dan nilai MSE pada data latih dan data uji
"""

# Buat variabel mse yang isinya adalah dataframe nilai mse data train dan test pada masing-masing algoritma
mse = pd.DataFrame(columns=['train', 'test'], index=['KNN','RF','Boosting'])

"""

*   Membuat kamus data untuk setiap algoritma yang digunakan"""

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'KNN': knn_best, 'RF': rf_regressor_best, 'Boosting': adaboost_regressor_best}

"""

*   Menghitung nilai MSE pada masing - masing algoritma pada data latih dan data uji
"""

# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e3
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e3

"""

*   Menampilkan tabel data yang memuat rangkuman nilai MSE pada pelatihan tiap algoritma menggunakan data latih dan data uji"""

# Panggil mse
mse

"""

*   Membuat dan menampilkan plot metrik hasil modelling menggunakan grafik batang"""

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""

*   Membuat prediksi menggunakan beberapa sampel fitur harga rumah"""

prediksi = X_test.iloc[:].copy()
pred_dict = {'y_true':y_test[:]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)

pd.DataFrame(pred_dict)
